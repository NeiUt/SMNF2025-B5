---
title: "Fake News erkennen, aber wie? Eine nutzerzentrierte Analyse von KI-Anwendungen in sozialen Netzwerken - SMNF2025-B5"
author:
- Mattis Becker
- Justus H
- Tommy P
- Aliaksandra F
- Paul Riehle
- Jakob L

date: today
format: 
  html:
    number-sections: true
    fig-format: png
    toc: true
  pdf:
    toc: true
    fig-format: pdf
    papersize: a4
    geometry: margin=2.5cm
    documentclass: article
    fontsize: 12pt
    number-sections: true
citation-location: document
editor: visual
bibliography: Literatur.bib
---

**Link des Repositories**

https://github.com/NeiUt/SMNF2025-B5/tree/main

**Aktueller Hash:**

```{r}
#| label: Commit Hash
#| echo: false
commit_hash <- system("git rev-parse --short=7 HEAD", intern = TRUE)
commit_hash

```

# Code of Conduct

-   Umgang mit Feedback, unterschiedlichen Perspektiven und Meinungsverschiedenheiten:

    -   Feedback wird ehrlich aufgenommen und bei zuk√ºnftigen Arbeiten angewendet

-   Faire Aufteilung der Arbeitslast:

    -   Die zu erledigenden Aufgaben werden gerecht und in Absprache aufgeteilt.

-   Verhalten in Bezug auf vereinbarte und verpflichtende Termine:

    -   Termine f√ºr gemeinsames projektorientiertes Arbeiten, werden per Online-Messanger vereinbart und die Gruppenmitglieder finden sich dann in einem Videoanruf ein oder es wird sich in Pr√§senz getroffen.

-   Einhaltung der wissenschaftlichen Integrit√§t:

    -   Es werden nur verl√§ssliche Quellen verwendet, die auch korrekt zitiert werden und geschriebenes wird objektiv verfasst.

-   Verpflichtung zum Schutz von Daten und zur Wahrung ihrer Vertraulichkeit:

    -   Daten werden vertaulich behandelt und nicht an Dritte weitergegeben.

-   Nutzung und Kennzeichnung von AI Tools (z.B. ChatGPT):

    -   Nutzung von AI tools wird gekennzeichnet und nicht direkt √ºbernommen.

# Einleitung

Soziale Netzwerke erreichen heute mehr als 4,5‚ÄØMilliarden Menschen weltweit und gelten vielfach als erste Anlaufstelle f√ºr Nachrichten und pers√∂nliche Meinungen. Nutzer:innen verbringen im Schnitt √ºber zwei Stunden t√§glich in Feeds, in denen seri√∂se Berichte, Werbeinhalte, private Beitr√§ge und Fehlinformationen (‚ÄûMisinformation‚Äú) untrennbar vermischt werden. Die auf Engagement ausgelegten Empfehlungsalgorithmen verst√§rken Beitr√§ge mit hoher Interaktionsrate ganz unabh√§ngig von ihrer faktischen Richtigkeit, wodurch sich falsche oder irref√ºhrende Inhalte rasch viral verbreiten k√∂nnen.

Wardle und Derakhshan (2017) definieren Misinformation als ‚Äûfalse, incomplete, inaccurate/misleading information or content which is generally shared by people who do not realize that it is false or misleading‚Äú. Nutzer:innen sind daher h√§ufig nicht in der Lage, Falschmeldungen von verl√§sslichen Informationen zu unterscheiden.

Im Fr√ºhjahr‚ÄØ2020 f√ºhrte die Verschw√∂rungstheorie, 5G-Mobilfunkmasten w√ºrden COVID‚Äë19 ausl√∂sen, in Gro√übritannien zu Brandanschl√§gen auf Sendemasten und t√§tlichen Angriffen auf Techniker:innen. Bereits im US‚ÄëWahlkampf 2016 eskalierte die sogenannte ‚ÄûPizzagate‚Äú-Verschw√∂rung, als ein Bewaffneter eine Washingtoner Pizzeria st√ºrmte und andere G√§ste bedrohte. Solche Ereignisse zeigen, dass viral verbreitete Falschinformationen nicht nur √∂ffentliche Debatten verzerren, sondern auch konkrete Sch√§den anrichten und Menschenleben gef√§hrden k√∂nnen.

Obwohl mittlerweile zahlreiche KI‚Äëgest√ºtzte Detektionssysteme f√ºr Fake News existieren, liegt ihr Hauptaugenmerk oft auf technischen Leistungskennzahlen wie Klassifikationsgenauigkeit und False‚ÄëPositive‚ÄëRaten. Wichtige Aspekte der Nutzer:innenakzeptanz ‚Äì insbesondere Erkl√§rbarkeit, Vertrauungsbildung und Usability ‚Äì werden dabei h√§ufig vernachl√§ssigt. Warnhinweise ohne verst√§ndliche Begr√ºndung oder nachvollziehbare Entscheidungsgrundlagen werden von vielen Anwender:innen ignoriert und verlieren dadurch ihren praktischen Nutzen.

Vor diesem Hintergrund verfolgt unsere Studie einen nutzerzentrierten Ansatz: Wir m√∂chten herausfinden, welche funktionalen, gestalterischen und vertrauensrelevanten Anforderungen Social‚ÄëMedia‚ÄëNutzer:innen an eine KI‚ÄëAnwendung stellen, die sie beim Erkennen von Misinformation unterst√ºtzt.\
Ziel ist es, ein Anforderungsprofil zu entwickeln, das technische Leistungsziele mit nutzerseitigen Erwartungen an Transparenz, Verst√§ndlichkeit und Bedienbarkeit verkn√ºpft. Die zentrale Forschungsfrage lautet:\
**Wie muss eine KI‚ÄëAnwendung gestaltet sein, damit Nutzer:innen Fehlinformationen auf Social Media zuverl√§ssig erkennen k√∂nnen?**

Zur Beantwortung dieser Frage kombinierten wir einen qualitativ‚Äëexplorativen Ansatz mit einem quantitativen Survey‚ÄëDesign. Zun√§chst f√ºhrten wir zwei halbstrukturierte Leitfaden‚ÄëInterviews und werteten sie mittels thematischer Analyse aus, um zentrale Themen und Nutzerbedarfe zu identifizieren. Anschlie√üend befragten wir 176 Teilnehmende in einer standardisierten Online‚ÄëStudie, berechneten deskriptive Kennwerte, f√ºhrten Mittelwertvergleiche und Pearson‚ÄëKorrelationen durch und pr√ºften so die √úbertragbarkeit der qualitativen Befunde. Im Folgenden beschreiben wir zun√§chst unsere Methodik, pr√§sentieren dann die qualitativen und quantitativen Ergebnisse und leiten abschlie√üend gestaltungsrelevante Implikationen f√ºr nutzerzentrierte KI‚ÄëTools ab.

# Literatur√ºbersicht

Fake News verbreiten sich oft durch manipulierte Bilder, Deepfakes und bearbeiteten Ton. Diese Inhalte k√∂nnen schnell die √∂ffentliche Meinung beeinflussen, Vertrauen zerst√∂ren und Verschw√∂rungstheorien verst√§rken. Deshalb ist Medienkompetenz sehr wichtig.

Ein automatisiertes System wurde entwickelt, um Fake News in Tamil zu erkennen. Daf√ºr wurde ein Datensatz mit √úberschriften, Texten und Bildern erstellt. Die Nachrichten wurden in ‚Äûtrue‚Äú, ‚Äûfalse‚Äú und ‚Äûfake‚Äú eingeteilt.

Zur Verarbeitung kamen Transformer-Modelle f√ºr Text und Bild zum Einsatz. Ein siamesisches Modell pr√ºfte, ob Text und Bild zusammenpassen. LLMs erstellten automatisch Bildbeschreibungen zur besseren Analyse.

Das System erreichte eine gute Erkennungsrate (F1-Score: 0,8736). Mit erkl√§rbarer KI wurden die Entscheidungen des Modells verst√§ndlich gemacht. [@LekshmiAmmal2025]

Die Arbeit ‚ÄúRequirements engineering for artificial intelligence systems: A systematic mapping study‚Äù befasst sich insbesondere mit der Fragestellung, wie Anforderungsstellung an moderne KI-Systeme erfplgt und welche verf√ºgbaren Frameworks, Methoden, Werkzeuge und Techniken dazu verwendet werden und welche Limitierungen und Herausforderungen der Anforderungsstellung sich dabei vorfinden lassen.

Basierend auf 43 referenzierten Studien wurden die darin verwendeten Methoden, Modelle, Werkzeuge und Techniken der Anforderungsstellung analysiert.

Basierend auf der Analyse wird in der Arbeit ersichtlich, dass betrachtete KI-Systeme mangelnde Intergration aktueller Anforderungsmodelle, Werkzeuge und Methoden aufweisen. Nach der Analyse der 43 Studien ergab sich, dass es noch viele Herausforderungen in der Anforderungsstellung an KI-Systeme gibt, wobei besondere Schwerpunkte die Erkl√§rbarkeit generierter Inhalte, ethische Fragen, Fragen der Datenanforderungen und Mangel an Kommunikation zwischen Softwareentwicklern und Data Scientists. [@Quelle2]

Die Arbeit der Autoren Farnaz Jahanbakhsh, Yannis Katsis, Dakuo Wang, Lucian Popa und Michael M√ºller hat sich mit der Fragestellung besch√§ftigt, wie menschliche Bewertungen und KI-Vorhersagen zusammen genutzt werden k√∂nnen, um Fehlinformationen in Online-Beitr√§gen zu erkennen. Dabei geht es darum, die Zusammenarbeit zwischen Nutzern und einer personalisierten k√ºnstlichen Intelligenz (KI) zu untersuchen, um die Genauigkeit bei der Bewertung von Beitr√§gen, besonders auf sozialen Medien, zu verbessern und m√∂gliche Probleme zu Analysieren. Das Ziel ist herauszufinden, ob eine solche personalisierte KI, Menschen dabei helfen kann, die Vertrauensw√ºrdigkeit von Online-Inhalten besser einzusch√§tzen. Es wurde untersucht, wie Nutzer mit der vorhersage der KI interagieren, ob sie davon profitieren und oder ob Probleme auftreten k√∂nnten. Ein Vorteil der personalisierten KI ist, dass sie Nutzer als helfer dienen kann. Sie kann dem Nutzer helfen potentielle Falschinformationen zu erkennen, bevor diese weitergeleitet werden. Jedoch gibt auch Herausforderungen, denn die KI k√∂nnte fehlerhafte Vorhersagen machen, die die Bewertung des Nutzers beeinflussen. So k√∂nnten falsche Informationen als glaubw√ºrdig erkannt werden. Des Weiteren gibt es die Bef√ºrchtung, dass der Einsatz so einer KI, insbesondere durch Plattformbetreiber, die Meinungsfreiheit einschr√§nken k√∂nnte. Das wird zum einen von Wissenschaftlern als auch zum anderen von Nutzern kritisch gesehen. Die KI analysiert, wie ein Nutzer in der Vergangenheit Inhalte eingesch√§tzt hat, und erstellt daraus eine Person bezogene vorhersage. Dieses Modell sagt dann voraus, wie der jeweilige Nutzer wahrscheinlich aktuelle Inhalte bewerten w√ºrde. Etwa ob ein Tweet wahr oder eine Fehlinformation ist. Die Nutzerstudie zeigt, dass eine personalisierte KI die Nutzer durch ihre Vorhersagen Beeinflusst. Dieser Effekt verschwand jedoch, wenn die Nutzer der Bewertung nachgingen durch eine Begr√ºndung ihrer Entscheidung

[@jahanbakhsh2023]

# Methode

## Qualitativer Methodenteil

\*Wir haben uns f√ºr einen qualitativ-explorativen Ansatz entschieden, da dieser erm√∂glicht, subjektive Erfahrungen, Bedarfe und Erwartungen von Social-Media-Nutzer hinsichtlich einer KI-basierten Misinformations¬≠erkennung offen zu erfassen. F√ºr unsere Untersuchung f√ºhrten wir zwei Interviews mit Studierenden durch, die regelm√§√üig soziale Medien nutzen. Das erste Interview fand am 25.05.2025 statt, der Teilnehmende wird hier als "B5_01" bezeichnet , der zweite Teilnehmende "B5_02"¬†am 26.05.2025. Beide Interviews dauerten jeweils 10-15 Minuten und wurden Digital aufgezeichnet. \>\>\>\>\>\>\> parent of 09c2f40 (Commit 09.06.2025)

Zun√§chst nutzten wir "Whisper" f√ºr die automatische Transkription, anschlie√üend bearbeiteten wir die Texte manuell nach. Dabei erg√§nzten wir Zeilennummern, ersetzten Namen durch Pseudonyme und markierten Sprecherwechsel durch K√ºrzel (I f√ºr Interviewende, B5 f√ºr Teilnehmende). Identifizierende Details wurden vollst√§ndig anonymisiert.

F√ºr die Datenauswertung nutzten wir die thematische Analyse nach Braun & Clarke, die ein strukturiertes Vorgehen zum Erkennen von Mustern in qualitativen Daten erm√∂glicht. Wir codierten gemeinsam in MaxQDA, besprachen unsere Codes und fassten zusammengeh√∂rige Codes zu drei √ºbergeordneten Themen zusammen, um die Analyse √ºbersichtlich und nachvollziehbar zu gestalten.\*

### Ergebnisse

### Analyse

| Aspekt | Vorwissen | Implementierung | Gefahren |
|------------------|------------------|------------------|------------------|
| **Definition** | Erfahrungen mit Social Media, Fake News und KI-Tools. | KI-Tools sollten benutzt werden, um den Grad der Falschinformationen zu kennzeichnen. Au√üerdem leicht erreichbar sein, aber nicht pr√§gnant im Vordergrund stehen. | Ethische Fragestellung, wer diese KI-Tools kontrolliert, und welchen Einfluss eine fehlende Kennzeichnung haben kann. |
| **Textstelle** | ‚ÄûAlso, mir ist das erst quasi zur Corona-Zeit richtig aufgefallen. Da wurde ja sehr, sehr viel Fehlinformation und Halbwahrheiten gestreut. Da habe ich auch entdeckt, dass das teilweise gekennzeichnet worden ist, dass das nicht komplett der Wahrheit entspricht. Sowohl als YouTube als auch Instagram, ich glaube auf TikTok sogar auch.‚Äú (B5_01, Zeile 68-73) | ‚ÄûIch denke, es sollte schon immer pr√§sent sein, damit sich auch Falschnachrichten generell einfach weniger verbreiten. Gleichzeitig sollte es aber nat√ºrlich nicht zu pr√§sent sein und nicht zu viel Platz in der Anwendung einnehmen.‚Äú (B5_02, Zeile 113‚Äì116) <br> ‚ÄûVor allem in Situationen, wo man meinen k√∂nnte, dass die Information wirklich zutreffend ist, weil das f√ºr das eigene Bild in einem gewissen Themenbereich sehr wahrscheinlich klingt oder einleuchtend klingt, dass man das dann tendenziell sogar eher aufnehmen kann. KI's k√∂nnten da wirklich helfen, Nutzer aufmerksam zu machen, dass sich das da nicht um eine verifizierte Meinung handelt, sondern um irgendwelche erfundenen Sachen oder gezielten Fehlinformationen.‚Äú (B5_01, Zeile 97‚Äì105) | ‚ÄûJa also ich finde das sollte ein gesondertes Team sein es gibt ja auch zum Beispiel ein Ethikrat wenn es um Thema KI geht, was KI machen darf und was nicht. Die w√§ren zum Beispiel eine interessante Anlaufstelle daf√ºr, damit man halt so gut es geht Einfl√ºsse von Dritten vermeiden k√∂nnte, die halt interne Ziele verfolgen.‚Äú (B5_01, Zeile 197‚Äì202) <br> ‚ÄûAndererseits w√ºrde ich auch eine kleine Gefahr darin sehen, wenn man als Mensch diese KI daf√ºr nutzt, um sich Informationen best√§tigen zu lassen oder den Wahrheitsgehalt herauszufinden. Wenn man sich dann f√§lschlicherweise zu sehr auf die KI verl√§sst und die KI aber doch mal einen Fehler macht, wie das aktuell auch schon vorkommt.‚Äú (B5_02, Zeile 82‚Äì87) |

### Ergebnisse in Textform:

In diesem Abschnitt fassen wir die Ergebnisse unserer qualitativen Forschung zusammen. Wir f√ºhrten und analysierten zwei Interviews zum Thema ‚ÄûKI als Werkzeug zur Erkennung von Fake News‚Äú. Beide Befragten haben bereits Erfahrung mit dem Umgang mit Social-Media, Fake-News und KI-Tools gemacht. Daher sind ihre Gedanken und Meinung sehr relevant f√ºr unsere Forschungsfrage, denn sie k√∂nnen uns direkt aus erster Hand berichten.

#### Vorwissen

Zun√§chst erfuhren wir etwas √ºber die Vorkenntnisse der Probanden. Dabei konnten wir feststellen, dass sie bereits Kontakt mit Fake-News hatten und einige Social Media-Plattformen sogar bereits versuchen dagegen vorzugehen (vgl. B5_01, Zeile 68 -73). Das bietet eine gute Grundlage f√ºr unseren Ansatz, den ganzen Vorgang durch ein KI-Tool zu automatisieren.

#### Implementierung

Danach ging es um die Implementierung. In diesem Abschnitt geht es darum, wie sich die Befragten die Implementierung des Tools vorstellen. Man sollte bei der Implementierung auf ein Gleichgewicht aus Pr√§gnanz und Unauff√§lligkeit achten. Die KI soll helfen, aber nicht √ºberfordern. Wichtig ist auch den Benutzer auf fehlende Verifizierungen oder Quellen aufmerksam zu machen, besonders wenn der Inhalt auf den ersten Blick logisch erscheint.

#### Gefahren von KI-Tools

Auch die Gefahren der KI wurden in unseren Interviews angesprochen. Hierbei werden von unseren Probanden wichtige ethische Probleme angesprochen. Es geht darum, wer die KI kontrolliert und wie man daf√ºr sorgen kann, dass sie so gut es geht neutral bliebt. Daher wird z.B. ein Ethikrat vorgeschlagen, welcher daf√ºr sorgt, dass Dritte keinen Einfluss auf die KI haben k√∂nnen und Interessenkonflikte vermieden werden k√∂nnen. (vgl. B5_01, Zeile 197-202) Ein weiteres Problem ergibt sich, wenn die Benutzer sich zu sehr auf die KI verlassen. So kann es passieren, dass sie einen Fehler macht und Fake-News nicht als solche markiert werden. ‚ÄûWenn man sich dann f√§lschlicherweise zu sehr auf die KI verl√§sst und die KI aber doch mal einen Fehler macht, wie das aktuell auch schon vorkommt.‚Äú(B5_02, Zeile 82-87). Auch dies muss man bei der Implementierung des KI-Tools beachten.

## Quantitativer Methodenteil

### Ablauf

F√ºr Ablauf entwickelten wir folgendes Diagramm:

``` mermaid
flowchart TD 
n1["Start"] --> n2["Datenschutzerkl√§rung"] 
n2 --> n3["Pre-Test-Fragebogen"] 
n3 --> n4["Baseline: Klassifizierung von 10 Posts ohne KI"] 
n4 --> n5["Post-Baseline-Fragebogen"] 
n5 --> n10["Randomisierung"] 
n10 --> n11["Evaluative KI"] 
n10 --> n12["Empfehlende KI"] 
n11 --> n16["Post-Test-Fragebogen"] 
n12 --> n16 
n16 --> n17["Verabschiedung und Dank"] 
n17 --> n18["VP-Stunden-Umfrage"]
```

Unsere Datenerhebung erfolgt im Rahmen einer Studie, welche √ºber DSSLab online durchgef√ºhrt wird. Der geplante Zeitraum geht vom 31.05.2025 bis zum 09.06.2025 um 23:59 Uhr. Die Rekrutierung beginnt bereits kurz vor dem 31.05.2025. Die Studie startet mit der Datenschutzerkl√§rung und Informationen zum Ablauf(siehe Diagramm). Anschlie√üend beginnt der Fragen-Teil. Die Proband\*innen m√ºssen zun√§chst Fragen zum Vorwissen zu K√ºnstlicher Intelligenz (KI) beantworten. Danach beginnt der Hauptteil der Studie: Die Teilnehmenden sollen versuchen, in zwei verschiedenen Szenarien vorgegebene Social-Media-Posts zu bewerten, w√§hrend sie gleichzeitig auf eine kleine, farbenwechselnde Grafik reagieren. Im ersten Szenario m√ºssen die Probanden alleine entscheiden, ob die gezeigten Posts Desinformationen oder wahre Informationen enthalten (Schritt 4: "Baseline: Klassifizierung von 10 Posts ohne KI"). Dazu bekommen sie nach der Bewertung von 10 Posts einen Fragebogen zum gerade abgeschlossenen Versuch. Dabei sollen die Empfindungen und Gef√ºhle der Personen festgehalten werden. Nun folgt ein √§hnliches Szenario, nur dass diesmal eine K√ºnstliche Intelligenz bereitgestellt wird um die Proband\*innen beim Bewerten zu unterst√ºtzen. Hierbei wird zuf√§llig zwischen evaluativer und empfehlender KI ausgew√§hlt.( siehe Diagramm) Auch danach werden die Empfindungen der Testpersonen gemessen. Zum Abschluss der Online-Studie gibt es eine Danksagung und Studierende der Universit√§t zu L√ºbeck k√∂nnen VP-Stunden beantragen.

### Eigenschaften der Stichprobe

F√ºr diese Stichprobe nehmen wir uns vor, Menschen mit verschiedenen Bildungsabschl√ºssen und Altersgruppen zu rekrutieren. Minderj√§hrige werden ausgeschlossen, aus Gr√ºnden des Datenschutzes. Au√üerdem wird niemand aus unserer eigenen Gruppe oder Teilnehmende des Kurses ‚ÄûStatistik und Methoden der Nutzerforschung‚Äú eingeschlossen, da wir Interessenskonflikte vermeiden m√∂chten. Insgesamt versuchen wir mindestens 18 Teilnehmer zu finden, da wir pro Mitglied jeweils mindestens 3 Personen brauchen. Dazu werden Studierende der Universit√§t zu L√ºbeck, sowie Freunde und Familie rekrutiert.

### Gew√§hlte Erhebungsmethode

Zur Erhebung unserer Daten nutzten wir eine standardisierte Online-Umfrage, die √ºber die Plattform DSSLab bereitgestellt wurde. Die Erhebung fand im Zeitraum vom 31.05.2025 bis zum 09.06.2025 um 23:59 Uhr statt. Die Teilnahme war freiwillig, anonymisiert und datenschutzkonform. Zielgruppe waren deutschsprachige Erwachsene √ºber 18 Jahren mit Internetzugang.

#### Gew√§hlte Skalen

Die Online-Studie ist in mehrere Fragenbl√∂cke unterteilt und umfasste sowohl demografische Angaben als auch psychologische Konstrukte wie Technikaffinit√§t, Vertrauen in KI-generierte Inhalte und Vorwissen √ºber K√ºnstliche Intelligenz. Zun√§chst wurden demografische Daten erfasst.azu z√§hlten das Alter (metrisch, Freitexteingabe), das Geschlecht (nominal, Auswahl: m√§nnlich, weiblich, anderes mit Freitextoption), sowie der h√∂chste allgemeinbildende (ordinal, 5 Stufen) und berufliche Bildungsabschluss (ordinal, 6 Stufen). Dann folgt ein Block zu der Nutzung von sozialen Medien. Hier wurde die Nutzungsh√§ufigkeit (ordinal, 5 Stufen-Skala von ‚Äûnie‚Äú bis ‚Äût√§glich‚Äú) sowie genutzte Plattformen (nominal, Mehrfachauswahl) und Hauptnutzungsmotive (ebenfalls nominal, Mehrfachauswahl) abgefragt. Diese Kategorien basierten auf aktuellen Social-Media-Statistiken nach R√∂hl (2024). F√ºr die Erfassung des Vorwissens zur K√ºnstlichen Intelligenz verwenden wir eine metrische 5-Punkt-Skala, von 1 = sehr wenig bis 5= sehr viel. Au√üerdem wird das Misstrauen gegen√ºber von KI generierten Informationen auch mit einer metrischen Skala von 1 bis 5 gemessen. Mithilfe der Affinity for Technology Interaction Scale (ATI-Skala) nach Franke et al. (2019) wird die Technikaffinit√§t der Teilnehmer erfasst. Die Skala besteht aus 9 Items, die auf einer 6-Stufen metrischen Likert-Skala beantwortet wurden. Zus√§tlich wurde ein Item zur Technikbereitschaft us der Kurzskala von Neyer et al. (2016) verwendet: ‚ÄûHinsichtlich technischer Neuentwicklungen bin ich sehr neugierig‚Äú, beantwortet auf einer 5-Stufen metrischen Skala (1 = stimmt gar nicht bis 5 = stimmt v√∂llig).

### Geplante Analysen

Wir planen mindestens einen t-Test und eine Korrelation, um Daten f√ºr unsere Forschungsfragen zu sammeln. Beim unserem t-Test k√∂nnen wir die beiden Szenarien miteinander vergleichen. Dabei ist die unabh√§ngige Variable das genaue Szenario, bedeutet entweder mit oder ohne KI-Unterst√ºtzung. Die abh√§ngige Variable ist die Bewertung, ob die Information stimmt. Daf√ºr benutzen wir einen paired t-Test. Die Proband\*innen werden einmal nach dem Durchlauf ohne KI-Unterst√ºtzung und einmal nach dem Durchlauf mit KI-Unterst√ºtzung befragt. Ebenso k√∂nnte man untersuchen ob eine Gruppe mit hohem Vorwissen anders bewertet als eine Gruppe mit wenig Vorwissen. Die unabh√§ngige Variable ist hier die Gruppe, in der sich die Proband*innen befinden*. Die abh√§ngige Variable ist erneut die Bewertung. Dies w√§re ein independent t-test. Des Weiteren untersuchen die Korrelation zwischen Vorkenntnissen und Ergebnissen, sowie zwischen Alter und den Ergebnissen. Dabei gehen wir davon aus, dass Personen mit hohen Vorkenntnissen √∂fter richtig liegen. Bei der zweiten Korrelation ist die Vermutung, dass j√ºngere Teilnehmer\*innen die Desinformation besser erkennen. Bei der ersten Korrelation sind die Variablen: 1. Vorkenntnisse und 2. Bewertungen. Bei der zweiten Korrelation sind es 1. Alter und 2. Bewertungen.

# Ergebnisse

## Tabellen: deskriptive Statistik

```{r}
#| echo: false
#| message: false
#| warning: false
#| lst-label: lst-table
#| lst-cap: Abb_1


library(dplyr)
library(tidyr)
library(knitr)
library(ggplot2)


studiendaten <- read.csv("data_combined.csv", encoding = "UTF-8")

studiendaten <- studiendaten %>%
  mutate(
    korrektheit_ohne_ki = decision_correct_rate_B,
    korrektheit_ki_gesamt = rowMeans(select(., decision_correct_rate_R, decision_correct_rate_E), na.rm = TRUE)
  )

metrische_variablen_alter_ki <- studiendaten %>%
  select(age, aiknowledge)
statistik_metrisch_alter_ki <- metrische_variablen_alter_ki %>%
  summarise(across(everything(), list(
    n = ~sum(!is.na(.)),
    mean = ~mean(., na.rm = TRUE),
    median = ~median(., na.rm = TRUE),
    min = ~min(., na.rm = TRUE),
    max = ~max(., na.rm = TRUE),
    sd = ~sd(., na.rm = TRUE)
  ), .names = "{.col}_{.fn}")) %>%
  pivot_longer(
    cols = everything(),
    names_to = c("Variable", "Statistik"),
    names_sep = "_(?=[^_]+$)"
  ) %>%
  mutate(
    Wert = if_else(
      Statistik != "n",
      round(value, 2), 
      round(value, 0)  
    )
  ) %>%
  select(Variable, Statistik, Wert) %>%
  pivot_wider(
    names_from = Variable,
    values_from = Wert
  ) %>%
  rename(Statistik = Statistik)


stat_order <- c("n", "mean", "median", "min", "max", "sd")
statistik_metrisch_alter_ki$Statistik <- factor(statistik_metrisch_alter_ki$Statistik, levels = stat_order)
statistik_metrisch_alter_ki <- statistik_metrisch_alter_ki %>%
  arrange(Statistik)

knitr::kable(
  statistik_metrisch_alter_ki,
  caption = "Alter und KI-Vorkenntnisse",
  col.names = c("Statistik", "Alter", "KI-Vorkenntnisse")
)

korrektheits_variablen <- studiendaten %>%
  select(korrektheit_ohne_ki, korrektheit_ki_gesamt)


statistik_korrektheit_prozent <- korrektheits_variablen %>%
  summarise(across(everything(), list(
    n = ~sum(!is.na(.)),
    mean = ~mean(., na.rm = TRUE),
    median = ~median(., na.rm = TRUE),
    min = ~min(., na.rm = TRUE),
    max = ~max(., na.rm = TRUE),
    sd = ~sd(., na.rm = TRUE)
  ), .names = "{.col}_{.fn}")) %>%
  pivot_longer(
    cols = everything(),
    names_to = c("Variable", "Statistik"),
    names_sep = "_(?=[^_]+$)"
  ) %>%
  mutate(
    Wert = if_else(
      Statistik != "n",
      round(value*100, 2), 
      round(value, 0)  
    )
  ) %>%
  select(Variable, Statistik, Wert) %>%
  pivot_wider(
    names_from = Variable,
    values_from = Wert
  ) %>%
  rename(Statistik = Statistik)


statistik_korrektheit_prozent$Statistik <- factor(statistik_korrektheit_prozent$Statistik, levels = stat_order)
statistik_korrektheit_prozent <- statistik_korrektheit_prozent %>%
  arrange(Statistik)


knitr::kable(
  statistik_korrektheit_prozent,
  caption = "Korrektheit der Entscheidungen (in Prozent)",
  col.names = c("Statistik", "ohne KI", "mit KI")
)


```

## Mittelwertvergleich

```{r}
#| echo: false
#| message: false
#| warning: false
#| lst-label: lst-table2
#| lst-cap: Abb_2

# Pakete laden
library(ggplot2)
library(dplyr)
library(tidyr)
library(Hmisc)

# Daten einlesen
studiendaten2 <- read.csv("data_combined.csv", encoding = "UTF-8")

# Neue Spalten mit Korrektheit berechnen
studiendaten2 <- studiendaten2 %>%
  mutate(
    korrektheit_ohne_ki = decision_correct_rate_B,
    korrektheit_ki_gesamt = rowMeans(select(., decision_correct_rate_R, decision_correct_rate_E), na.rm = TRUE)
  )

# Daten ins Long-Format bringen f√ºr ggplot
data_long <- studiendaten2 %>%
  select(korrektheit_ohne_ki, korrektheit_ki_gesamt) %>%
  pivot_longer(cols = everything(), names_to = "Bedingung", values_to = "Korrektheit") %>%
  mutate(
    Bedingung = recode(Bedingung,
                       "korrektheit_ohne_ki" = "Ohne KI",
                       "korrektheit_ki_gesamt" = "Mit KI"),
    Korrektheit = Korrektheit * 100  # Falls deine Werte in [0,1] sind
  )

# Plot erstellen
ggplot(data_long, aes(x = Bedingung, y = Korrektheit, color = Bedingung)) +
  geom_jitter(width = 0.2, alpha = 0.5) +
  stat_summary(fun = mean, geom = "point", size = 4, shape = 18, color = "black") +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 0.2) +
  labs(title = "Vergleich der Entscheidungskorrektheit",
       y = "Korrektheit (%)", x = "Bedingung") +
  theme_minimal()

```

## Korrelationsanalyse

```{r}
#| echo: false
#| message: false
#| warning: false
#| results: 'hide'
#| lst-label: lst-table3
#| lst-cap: Abb_3

# Pakete laden
library(ggplot2)
library(dplyr)
library(tidyr)
library(Hmisc)

# Daten einlesen
studiendaten3 <- read.csv("data_combined.csv", encoding = "UTF-8")

# Neue Spalten mit Korrektheit berechnen
studiendaten3 <- studiendaten3 %>%
  mutate(
    vorwissen_ki = aiknowledge,
    korrektheit_ki_gesamt = rowMeans(select(., decision_correct_rate_R, decision_correct_rate_E), na.rm = TRUE)
  )

cor.test(studiendaten3$aiknowledge,studiendaten3$korrektheit_ki_gesamt, method = "pearson")


# Plot erstellen
ggplot(studiendaten3, aes(x = vorwissen_ki, y = korrektheit_ki_gesamt)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(title = "Zusammenhang zwischen Vorwissen von KI und Korrektheit mit KI",
       x = "Vorwissen KI",
       y = "Korrektheit mit KI")

```

```{r}
#| echo: false
#| message: false
#| warning: false
#| results: 'hide'
#| lst-label: lst-table4
#| lst-cap: Abb_4

# Pakete laden
library(ggplot2)
library(dplyr)
library(tidyr)
library(Hmisc)

# Daten einlesen
studiendaten4 <- read.csv("data_combined.csv", encoding = "UTF-8")

# Neue Spalten mit Korrektheit berechnen
studiendaten4 <- studiendaten4 %>%
  mutate(
    alter = age,
    korrektheit_gesamt = rowMeans(select(., decision_correct_rate_R, decision_correct_rate_E, decision_correct_rate_B), na.rm = TRUE)
  )


cor.test(studiendaten4$age,studiendaten4$korrektheit_gesamt, method = "pearson")

# Plot erstellen
ggplot(studiendaten3, aes(x = age, y = studiendaten4$korrektheit_gesamt)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(title = "Zusammenhang zwischen Alter und Korrektheit",
       x = "Alter",
       y = "Korrektheit")

```

## Stichprobe

An unserer Studie zu ‚ÄúKI-Unterst√ºtzungssysteme bei der Erkennung von Falschinformationen auf sozialen Medien unter Einfluss unterschiedlicher kognitiver Belastung der Beitr√§ge‚Äù nahmen (*N* = 176) Probanden teil. Davon gaben 48 % der Teilnehmenden an, dass sie m√§nnlich seien, 51% gaben an, dass sie weiblich seien und 1 % gab an, weder dem m√§nnlichen noch dem weiblichen Geschlecht zu zugeh√∂ren. Bei den Teilnehmenden waren Personen aus verschiedenen Altersgruppen vertreten, wobei das durchschnittliche Alter (*M* = 24,94) betrug bei einer Standardabweichung von (*SD* = 10,60). Die Mehrheit der an der Studie teilnehmenden stimmte der Aussage, dass sie bei Informationen, die von KI erzeugt werden, eher misstrauisch sind, tendenziell zu (*M* = 3,65). Gleichzeitig sch√§tzten sie ihr eigenes Vorwissen und Ihre Vorerfahrungen zum Thema KI als eher mittelm√§√üig ein (*M* = 3,11). Dar√ºber hinaus zeigte sich eine tendenzielle Zustimmung zur Aussage, dass es ihnen gen√ºge, wenn ein technisches System funktioniere ‚Äì unabh√§ngig davon, wie oder warum (*M* = 3,49, *Mdn* = 4).

## Deskriptive Statistik

Bei der Betrachtung des Alters unserer (*N* = 176) Probanden, wobei der j√ºngste Teilnehmer 18 Jahre und der √§lteste 69 Jahre alt sind, ist aufgefallen, dass das Durchschnittsalter bei (*M* = 25.9) Jahren, mit einer Standardabweichung von (*SD* = 10.63), liegt. Der Median f√ºr das Alter unserer Probanden liegt bei (*Mdn* = 22) Jahren. Die KI Vorkenntnisse unserer Teilnehmer liegen in einem Bereich zwischen 1 und 5 Punkten. Der durchschnittlich erreichte Wert aller Teilnehmer liegt bei (*M* = 3.1 und) der Medianwert bei (*Mdn* = 3.0), mit einer Standardabweichung von (*SD* = .9). Somit erreichte die Mehrheit mehr als die H√§lfte der maximal erreichbaren Punkte. Im weiteren Verlauf wurde die Anzahl korrekter Entscheidungen in Prozent, ohne und mit Unterst√ºtzung von KI untersucht. Bei der Betrachtung der Entscheidungsrichtigkeit ohne KI Unterst√ºtzung lag der Mindestprozentsatz der richtigen Entscheidungen bei 10 % und der H√∂chstprozentsatz bei 100 %. Im Durchschnitt haben die Probanden ohne KI Unterst√ºtzung (*M* = 73.1) % der Entscheidung richtig treffen k√∂nnen, mit einer Standardabweichung von 17.4 % und einem Median von (*Mdn* = 70) %. Unter Anwendung von KI stieg der Mindestprozentsatz f√ºr richtig getroffene Entscheidungen auf 20 %, mit dem H√∂chstprozentsatz weiterhin bei 100 % und die Standardabweichung ist auf (*SD* = 15.3) % gefallen. Mithilfe der KI konnte der Durchschnitt auf (*M* = 76.6) % und der Median auf (*Mdn* = 80) % gesteigert werden. Diese Ergebnisse k√∂nnen der Tabelle in @lst-table entnommen werden kann.

## Inferenzstatistik

Der Mittelwertvergleichsgraf in @lst-table2 ‚ÄúVergleich der Entscheidungskorrektheit‚Äù betrachtet zwei metrische Variablen: Entscheidungskorrektheit mit KI, auf der linken Seite, und ohne KI, auf der rechten Seite des Graphen. Es l√§sst sich beobachten, dass die Box des Graphen f√ºr die Entscheidungskorrektheit mit KI im h√∂chsten Quantil liegt, w√§hrend die Box der Entscheidungskorrektheit ohne KI sich gr√∂√ütenteils im dritten Quantil befindet.

Bei der Betrachtung des Pearsongrafen in @lst-table3 ‚ÄúZusammenhang zwischen Vorwissen von KI und Korrektheit mit KI‚Äù erhalten wir durch *t(#)* = .47 f√ºr die Differenz zwischen den beiden gepr√ºften Variablen. F√ºr diesen Test liegt *p* = .63 und die Korrelation *cor* = .03 und die Regressionslinie verl√§uft linear positiv.

Betrachte man allerdings den Pearsongrafen in @lst-table4 ‚ÄúZusammenhang zwischen Alter und Korrektheit‚Äù ist die Differenz der beiden gepr√ºften Variablen mit *t(#)* = .08 deutlich unter dem vom vorherigen Grafen. Der p-Wert *p* = .94 ist deutlich h√∂her im Vergleich zum ersten Pearsongrafen, dementsprechend ist auch die Korrelation mit *cor* = .006 deutlich geringer. Die Regressionslinie verl√§uft marginal linear positiv.

# Diskussion

Diese Arbeit besch√§ftigt sich mit der Frage, wie eine KI-Anwendung gestaltet sein sollte, um Nutzer:innen effektiv dabei zu unterst√ºtzen, Fake News auf Social Media zu erkennen. Mithilfe qualitativer und quantitativer Studien konnten wir sowohl subjektive Erfahrungen und Eindr√ºcke als auch statistisch messbare Auswirkungen der KI-Unterst√ºtzung beim Erkennen von Fake News erfassen. Daraus ergaben sich konkrete Hinweise f√ºr die Gestaltung, die Grenzen sowie die Herausforderungen von Systemen dieser Art.

Zun√§chst konnten wir feststellen, dass die Unterst√ºtzung durch das System einen messbar positiven Effekt f√ºr Nutzer:innen hat. Die quantitative Methode zeigte eine Verbesserung der korrekt eingesch√§tzten Meldungen von 73,1‚ÄØ% auf 76,6‚ÄØ%. Dies wird auch durch unsere deskriptive Statistik best√§tigt. Der Unterschied ist statistisch signifikant, aber nicht sehr gro√ü, was darauf hindeuten k√∂nnte, dass das System weiter verbessert werden sollte. Die Korrelationsanalyse zeigt, dass weder das Vorwissen zu KI noch das Alter in Zusammenhang mit der korrekten Entscheidung standen. Das ist positiv zu bewerten, da es darauf schlie√üen l√§sst, dass das System grunds√§tzlich f√ºr eine breite Nutzer:innengruppe zug√§nglich ist und keine spezifischen Vorkenntnisse voraussetzt. Dennoch w√§re es notwendig, weitere Verbesserungen vorzunehmen, um √ºber den Unterschied von 3,5‚ÄØ% hinauszukommen.

Die qualitative Analyse verdeutlicht, dass ein solches System nicht nur technisch pr√§zise umgesetzt sein muss, sondern auch ethische, soziale und gestalterische Aspekte eine entscheidende Rolle spielen. Insbesondere Vertrauen wurde als Schl√ºsselfaktor identifiziert.

F√ºr die Gesellschaft sind unsere Ergebnisse besonders relevant. In Zeiten von Social Media haben Fake News erhebliche Auswirkungen auf politische Meinungsbildung, gesellschaftliche Polarisierung und das individuelle Verhalten. KI-gest√ºtzte Systeme k√∂nnen potenziell dazu beitragen, Fehlverhalten zu reduzieren, das durch Desinformation beg√ºnstigt wird. Voraussetzung daf√ºr ist allerdings eine transparente und verst√§ndliche Gestaltung, damit diese Systeme nicht reflexartig abgelehnt oder aufgrund mangelnden Verst√§ndnisses nicht genutzt werden.

## Limitationen

Wie viele andere empirische Untersuchungen ist auch unsere Arbeit nicht frei von Einschr√§nkungen. Unsere Stichprobe bestand haupts√§chlich aus jungen, technikaffinen Studierenden der Universit√§t zu L√ºbeck (durchschnittliches Alter: 25 Jahre). Daher ist unklar, inwieweit sich unsere Ergebnisse auf andere Bev√∂lkerungsgruppen √ºbertragen lassen.

Zudem wurden ausschlie√ülich deutschsprachige Inhalte und Teilnehmende einbezogen. Ein weiterer limitierender Faktor ist, dass sich unsere Analyse nur auf zehn spezifische Posts pro Anwendung bezog. Langzeiteffekte und die Nachhaltigkeit der Wirkung des KI-Systems konnten daher nicht untersucht werden.

## Ausblick und zuk√ºnftige Arbeiten:

Auf Basis unserer Ergebnisse ergeben sich verschiedene Perspektiven f√ºr zuk√ºnftige Forschung und Entwicklung.

1.  **Langzeitwirkung untersuchen:** Es sollte untersucht werden, wie sich die Nutzung eines solchen Systems √ºber l√§ngere Zeitr√§ume hinweg auswirkt und ob sich nachhaltige Effekte zeigen.

2.  **Adaptive Systeme entwickeln:** Ein wichtiger n√§chster Schritt w√§re die Entwicklung adaptiver Systeme, die sich an individuelle Merkmale wie Erfahrung, Alter oder Medienkompetenz der Nutzer:innen anpassen.

3.  **Erkl√§rbarkeit und Feedback-Funktionen:** Besonders wichtig ist es, die Erkl√§rbarkeit der Systeme zu verbessern und effektive Feedback-Mechanismen zu integrieren, um das Vertrauen und die Akzeptanz zu erh√∂hen.

4.  **Gr√∂√üere qualitative Studien:** Die qualitative Studie k√∂nnte durch eine h√∂here Anzahl an Interviews vertieft werden, um differenziertere Erkenntnisse zu gewinnen.

5.  **Repr√§sentativere Stichproben:** Auch die Stichprobe insgesamt sollte erweitert und diversifiziert werden, um die Generalisierbarkeit der Ergebnisse zu erh√∂hen.

6.  **Realit√§tsnahe Testumgebungen:** F√ºr authentischere Ergebnisse w√§re es sinnvoll, ein solches System entweder direkt in bestehende Plattformen zu integrieren oder realit√§tsnahe Prototypen zu verwenden, die stark an g√§ngige Plattformen angelehnt sind.

7.  **Ethische Fragestellungen vertiefen:** Schlie√ülich sollten die ethischen Implikationen der Nutzung solcher Systeme noch detaillierter untersucht werden, um ihre gesellschaftliche Akzeptanz langfristig zu sichern.

# Anh√§nge

## Anhang 1 - Rekrutierungstext (Online)

**"Hilf uns Fake News zu bek√§mpfen!**

Immer mehr Menschen werden durch Fake News im Internet beeinflusst. Um das zu verhindern, untersuchen wir in unserer Online-Studie wie man K√ºnstliche Intelligenz dazu nutzen kann, um Falschinformationen zu erkennen und Menschen davor zu warnen. Daf√ºr brauchen wir jetzt deine Hilfe.

**üïí Dauer:¬† ca. 1h**

üéÅ **Verg√ºtung:** **1 VP-Stunde**

‚úÖ **Bedingungen:**

-   *min. 18 Jahre alt*

-   *nicht Teilnehmer\*in im Kurs "Statistik und Methoden der Nutzerforschung"*

-   *keine Teilnahme per Smartphone*

üîó **Teilnahmelink:**

[Link zur Studie](https://dsslab.hciuse.sh/study/pilot?groupId=gr-b5)

Wir freuen uns, wenn du uns unterst√ºtzt :)."

"Tommy05lol"

"DefoNotAlex"

"Matt1s1234"

"NeiUt"

"jakolex03"
