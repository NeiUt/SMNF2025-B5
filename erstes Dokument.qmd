---
title: "SMNF2025-B5"
author:
- Mattis Becker
- Justus H
- Tommy P
- Aliaksandra F
- Paul Riehle
- Jakob L

date: today
format: 
  html:
    number-sections: true
    fig-format: png
    toc: true
  pdf:
    toc: true
    fig-format: pdf
    papersize: a4
    geometry: margin=2.5cm
    documentclass: article
    fontsize: 12pt
    number-sections: true
citation-location: document
editor: visual
bibliography: Literatur.bib
---

**Link des Repositories**

https://github.com/NeiUt/SMNF2025-B5/tree/main

**Aktueller Hash:**

```{r}
#| label: Commit Hash
#| echo: false
commit_hash <- system("git rev-parse --short=7 HEAD", intern = TRUE)
commit_hash

```

# Code of Conduct

-   Umgang mit Feedback, unterschiedlichen Perspektiven und Meinungsverschiedenheiten:

    -   Feedback wird ehrlich aufgenommen und bei zukünftigen Arbeiten angewendet

-   Faire Aufteilung der Arbeitslast:

    -   Die zu erledigenden Aufgaben werden gerecht und in Absprache aufgeteilt.

-   Verhalten in Bezug auf vereinbarte und verpflichtende Termine:

    -   Termine für gemeinsames projektorientiertes Arbeiten, werden per Online-Messanger vereinbart und die Gruppenmitglieder finden sich dann in einem Videoanruf ein oder es wird sich in Präsenz getroffen.

-   Einhaltung der wissenschaftlichen Integrität:

    -   Es werden nur verlässliche Quellen verwendet, die auch korrekt zitiert werden und geschriebenes wird objektiv verfasst.

-   Verpflichtung zum Schutz von Daten und zur Wahrung ihrer Vertraulichkeit:

    -   Daten werden vertaulich behandelt und nicht an Dritte weitergegeben.

-   Nutzung und Kennzeichnung von AI Tools (z.B. ChatGPT):

    -   Nutzung von AI tools wird gekennzeichnet und nicht direkt übernommen.

# Einleitung

*(Einleitung eingefügen)*

# Literaturübersicht

Fake News verbreiten sich oft durch manipulierte Bilder, Deepfakes und bearbeiteten Ton. Diese Inhalte können schnell die öffentliche Meinung beeinflussen, Vertrauen zerstören und Verschwörungstheorien verstärken. Deshalb ist Medienkompetenz sehr wichtig.

Ein automatisiertes System wurde entwickelt, um Fake News in Tamil zu erkennen. Dafür wurde ein Datensatz mit Überschriften, Texten und Bildern erstellt. Die Nachrichten wurden in „true“, „false“ und „fake“ eingeteilt.

Zur Verarbeitung kamen Transformer-Modelle für Text und Bild zum Einsatz. Ein siamesisches Modell prüfte, ob Text und Bild zusammenpassen. LLMs erstellten automatisch Bildbeschreibungen zur besseren Analyse.

Das System erreichte eine gute Erkennungsrate (F1-Score: 0,8736). Mit erklärbarer KI wurden die Entscheidungen des Modells verständlich gemacht. [@LekshmiAmmal2025]

Die Arbeit “Requirements engineering for artificial intelligence systems: A systematic mapping study” befasst sich insbesondere mit der Fragestellung, wie Anforderungsstellung an moderne KI-Systeme erfplgt und welche verfügbaren Frameworks, Methoden, Werkzeuge und Techniken dazu verwendet werden und welche Limitierungen und Herausforderungen der Anforderungsstellung sich dabei vorfinden lassen.

Basierend auf 43 referenzierten Studien wurden die darin verwendeten Methoden, Modelle, Werkzeuge und Techniken der Anforderungsstellung analysiert.

Basierend auf der Analyse wird in der Arbeit ersichtlich, dass betrachtete KI-Systeme mangelnde Intergration aktueller Anforderungsmodelle, Werkzeuge und Methoden aufweisen. Nach der Analyse der 43 Studien ergab sich, dass es noch viele Herausforderungen in der Anforderungsstellung an KI-Systeme gibt, wobei besondere Schwerpunkte die Erklärbarkeit generierter Inhalte, ethische Fragen, Fragen der Datenanforderungen und Mangel an Kommunikation zwischen Softwareentwicklern und Data Scientists. [@Quelle2]

Die Arbeit der Autoren Farnaz Jahanbakhsh, Yannis Katsis, Dakuo Wang, Lucian Popa und Michael Müller hat sich mit der Fragestellung beschäftigt, wie menschliche Bewertungen und KI-Vorhersagen zusammen genutzt werden können, um Fehlinformationen in Online-Beiträgen zu erkennen. Dabei geht es darum, die Zusammenarbeit zwischen Nutzern und einer personalisierten künstlichen Intelligenz (KI) zu untersuchen, um die Genauigkeit bei der Bewertung von Beiträgen, besonders auf sozialen Medien, zu verbessern und mögliche Probleme zu Analysieren. Das Ziel ist herauszufinden, ob eine solche personalisierte KI, Menschen dabei helfen kann, die Vertrauenswürdigkeit von Online-Inhalten besser einzuschätzen. Es wurde untersucht, wie Nutzer mit der vorhersage der KI interagieren, ob sie davon profitieren und oder ob Probleme auftreten könnten. Ein Vorteil der personalisierten KI ist, dass sie Nutzer als helfer dienen kann. Sie kann dem Nutzer helfen potentielle Falschinformationen zu erkennen, bevor diese weitergeleitet werden. Jedoch gibt auch Herausforderungen, denn die KI könnte fehlerhafte Vorhersagen machen, die die Bewertung des Nutzers beeinflussen. So könnten falsche Informationen als glaubwürdig erkannt werden. Des Weiteren gibt es die Befürchtung, dass der Einsatz so einer KI, insbesondere durch Plattformbetreiber, die Meinungsfreiheit einschränken könnte. Das wird zum einen von Wissenschaftlern als auch zum anderen von Nutzern kritisch gesehen. Die KI analysiert, wie ein Nutzer in der Vergangenheit Inhalte eingeschätzt hat, und erstellt daraus eine Person bezogene vorhersage. Dieses Modell sagt dann voraus, wie der jeweilige Nutzer wahrscheinlich aktuelle Inhalte bewerten würde. Etwa ob ein Tweet wahr oder eine Fehlinformation ist. Die Nutzerstudie zeigt, dass eine personalisierte KI die Nutzer durch ihre Vorhersagen Beeinflusst. Dieser Effekt verschwand jedoch, wenn die Nutzer der Bewertung nachgingen durch eine Begründung ihrer Entscheidung

[@jahanbakhsh2023]

# Methode

## Qualitativer Methodenteil

\*Wir haben uns für einen qualitativ-explorativen Ansatz entschieden, da dieser ermöglicht, subjektive Erfahrungen, Bedarfe und Erwartungen von Social-Media-Nutzer hinsichtlich einer KI-basierten Misinformations­erkennung offen zu erfassen. Für unsere Untersuchung führten wir zwei Interviews mit Studierenden durch, die regelmäßig soziale Medien nutzen. Das erste Interview fand am 25.05.2025 statt, der Teilnehmende wird hier als "B5_01" bezeichnet , der zweite Teilnehmende "B5_02" am 26.05.2025. Beide Interviews dauerten jeweils 10-15 Minuten und wurden Digital aufgezeichnet. \>\>\>\>\>\>\> parent of 09c2f40 (Commit 09.06.2025)

Zunächst nutzten wir "Whisper" für die automatische Transkription, anschließend bearbeiteten wir die Texte manuell nach. Dabei ergänzten wir Zeilennummern, ersetzten Namen durch Pseudonyme und markierten Sprecherwechsel durch Kürzel (I für Interviewende, B5 für Teilnehmende). Identifizierende Details wurden vollständig anonymisiert.

Für die Datenauswertung nutzten wir die thematische Analyse nach Braun & Clarke, die ein strukturiertes Vorgehen zum Erkennen von Mustern in qualitativen Daten ermöglicht. Wir codierten gemeinsam in MaxQDA, besprachen unsere Codes und fassten zusammengehörige Codes zu drei übergeordneten Themen zusammen, um die Analyse übersichtlich und nachvollziehbar zu gestalten.\*

### Ergebnisse

### Analyse

| Aspekt         | Vorwissen                                                                                                                                                                                                                                                                                                                                                            | Implementierung                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Gefahren                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
|-------------|-------------|------------------------|----------------------|
| **Definition** | Erfahrungen mit Social Media, Fake News und KI-Tools.                                                                                                                                                                                                                                                                                                                | KI-Tools sollten benutzt werden, um den Grad der Falschinformationen zu kennzeichnen. Außerdem leicht erreichbar sein, aber nicht prägnant im Vordergrund stehen.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Ethische Fragestellung, wer diese KI-Tools kontrolliert, und welchen Einfluss eine fehlende Kennzeichnung haben kann.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| **Textstelle** | „Also, mir ist das erst quasi zur Corona-Zeit richtig aufgefallen. Da wurde ja sehr, sehr viel Fehlinformation und Halbwahrheiten gestreut. Da habe ich auch entdeckt, dass das teilweise gekennzeichnet worden ist, dass das nicht komplett der Wahrheit entspricht. Sowohl als YouTube als auch Instagram, ich glaube auf TikTok sogar auch.“ (B5_01, Zeile 68-73) | „Ich denke, es sollte schon immer präsent sein, damit sich auch Falschnachrichten generell einfach weniger verbreiten. Gleichzeitig sollte es aber natürlich nicht zu präsent sein und nicht zu viel Platz in der Anwendung einnehmen.“ (B5_02, Zeile 113–116) <br> „Vor allem in Situationen, wo man meinen könnte, dass die Information wirklich zutreffend ist, weil das für das eigene Bild in einem gewissen Themenbereich sehr wahrscheinlich klingt oder einleuchtend klingt, dass man das dann tendenziell sogar eher aufnehmen kann. KI's könnten da wirklich helfen, Nutzer aufmerksam zu machen, dass sich das da nicht um eine verifizierte Meinung handelt, sondern um irgendwelche erfundenen Sachen oder gezielten Fehlinformationen.“ (B5_01, Zeile 97–105) | „Ja also ich finde das sollte ein gesondertes Team sein es gibt ja auch zum Beispiel ein Ethikrat wenn es um Thema KI geht, was KI machen darf und was nicht. Die wären zum Beispiel eine interessante Anlaufstelle dafür, damit man halt so gut es geht Einflüsse von Dritten vermeiden könnte, die halt interne Ziele verfolgen.“ (B5_01, Zeile 197–202) <br> „Andererseits würde ich auch eine kleine Gefahr darin sehen, wenn man als Mensch diese KI dafür nutzt, um sich Informationen bestätigen zu lassen oder den Wahrheitsgehalt herauszufinden. Wenn man sich dann fälschlicherweise zu sehr auf die KI verlässt und die KI aber doch mal einen Fehler macht, wie das aktuell auch schon vorkommt.“ (B5_02, Zeile 82–87) |

### Ergebnisse in Textform:

In diesem Abschnitt fassen wir die Ergebnisse unserer qualitativen Forschung zusammen. Wir führten und analysierten zwei Interviews zum Thema „KI als Werkzeug zur Erkennung von Fake News“. Beide Befragten haben bereits Erfahrung mit dem Umgang mit Social-Media, Fake-News und KI-Tools gemacht. Daher sind ihre Gedanken und Meinung sehr relevant für unsere Forschungsfrage, denn sie können uns direkt aus erster Hand berichten.

#### Vorwissen

Zunächst erfuhren wir etwas über die Vorkenntnisse der Probanden. Dabei konnten wir feststellen, dass sie bereits Kontakt mit Fake-News hatten und einige Social Media-Plattformen sogar bereits versuchen dagegen vorzugehen (vgl. B5_01, Zeile 68 -73). Das bietet eine gute Grundlage für unseren Ansatz, den ganzen Vorgang durch ein KI-Tool zu automatisieren.

#### Implementierung

Danach ging es um die Implementierung. In diesem Abschnitt geht es darum, wie sich die Befragten die Implementierung des Tools vorstellen. Man sollte bei der Implementierung auf ein Gleichgewicht aus Prägnanz und Unauffälligkeit achten. Die KI soll helfen, aber nicht überfordern. Wichtig ist auch den Benutzer auf fehlende Verifizierungen oder Quellen aufmerksam zu machen, besonders wenn der Inhalt auf den ersten Blick logisch erscheint.

#### Gefahren von KI-Tools

Auch die Gefahren der KI wurden in unseren Interviews angesprochen. Hierbei werden von unseren Probanden wichtige ethische Probleme angesprochen. Es geht darum, wer die KI kontrolliert und wie man dafür sorgen kann, dass sie so gut es geht neutral bliebt. Daher wird z.B. ein Ethikrat vorgeschlagen, welcher dafür sorgt, dass Dritte keinen Einfluss auf die KI haben können und Interessenkonflikte vermieden werden können. (vgl. B5_01, Zeile 197-202) Ein weiteres Problem ergibt sich, wenn die Benutzer sich zu sehr auf die KI verlassen. So kann es passieren, dass sie einen Fehler macht und Fake-News nicht als solche markiert werden. „Wenn man sich dann fälschlicherweise zu sehr auf die KI verlässt und die KI aber doch mal einen Fehler macht, wie das aktuell auch schon vorkommt.“(B5_02, Zeile 82-87). Auch dies muss man bei der Implementierung des KI-Tools beachten.

## Quantitativer Methodenteil

### Ablauf

Für Ablauf entwickelten wir folgendes Diagramm:

``` mermaid
flowchart TD 
n1["Start"] --> n2["Datenschutzerklärung"] 
n2 --> n3["Pre-Test-Fragebogen"] 
n3 --> n4["Baseline: Klassifizierung von 10 Posts ohne KI"] 
n4 --> n5["Post-Baseline-Fragebogen"] 
n5 --> n10["Randomisierung"] 
n10 --> n11["Evaluative KI"] 
n10 --> n12["Empfehlende KI"] 
n11 --> n16["Post-Test-Fragebogen"] 
n12 --> n16 
n16 --> n17["Verabschiedung und Dank"] 
n17 --> n18["VP-Stunden-Umfrage"]
```

Unsere Datenerhebung erfolgt im Rahmen einer Studie, welche über DSSLab online durchgeführt wird. Der geplante Zeitraum geht vom 31.05.2025 bis zum 09.06.2025 um 23:59 Uhr. Die Rekrutierung beginnt bereits kurz vor dem 31.05.2025. Die Studie startet mit der Datenschutzerklärung und Informationen zum Ablauf(siehe Diagramm). Anschließend beginnt der Fragen-Teil. Die Proband\*innen müssen zunächst Fragen zum Vorwissen zu Künstlicher Intelligenz (KI) beantworten. Danach beginnt der Hauptteil der Studie: Die Teilnehmenden sollen versuchen, in zwei verschiedenen Szenarien vorgegebene Social-Media-Posts zu bewerten, während sie gleichzeitig auf eine kleine, farbenwechselnde Grafik reagieren. Im ersten Szenario müssen die Probanden alleine entscheiden, ob die gezeigten Posts Desinformationen oder wahre Informationen enthalten (Schritt 4: "Baseline: Klassifizierung von 10 Posts ohne KI"). Dazu bekommen sie nach der Bewertung von 10 Posts einen Fragebogen zum gerade abgeschlossenen Versuch. Dabei sollen die Empfindungen und Gefühle der Personen festgehalten werden. Nun folgt ein ähnliches Szenario, nur dass diesmal eine Künstliche Intelligenz bereitgestellt wird um die Proband\*innen beim Bewerten zu unterstützen. Hierbei wird zufällig zwischen evaluativer und empfehlender KI ausgewählt.( siehe Diagramm) Auch danach werden die Empfindungen der Testpersonen gemessen. Zum Abschluss der Online-Studie gibt es eine Danksagung und Studierende der Universität zu Lübeck können VP-Stunden beantragen.

### Eigenschaften der Stichprobe

Für diese Stichprobe nehmen wir uns vor, Menschen mit verschiedenen Bildungsabschlüssen und Altersgruppen zu rekrutieren. Minderjährige werden ausgeschlossen, aus Gründen des Datenschutzes. Außerdem wird niemand aus unserer eigenen Gruppe oder Teilnehmende des Kurses „Statistik und Methoden der Nutzerforschung“ eingeschlossen, da wir Interessenskonflikte vermeiden möchten. Insgesamt versuchen wir mindestens 18 Teilnehmer zu finden, da wir pro Mitglied jeweils mindestens 3 Personen brauchen. Dazu werden Studierende der Universität zu Lübeck, sowie Freunde und Familie rekrutiert.

### Gewählte Erhebungsmethode

Zur Erhebung unserer Daten nutzten wir eine standardisierte Online-Umfrage, die über die Plattform DSSLab bereitgestellt wurde. Die Erhebung fand im Zeitraum vom 31.05.2025 bis zum 09.06.2025 um 23:59 Uhr statt. Die Teilnahme war freiwillig, anonymisiert und datenschutzkonform. Zielgruppe waren deutschsprachige Erwachsene über 18 Jahren mit Internetzugang.

#### Gewählte Skalen

Die Online-Studie ist in mehrere Fragenblöcke unterteilt und umfasste sowohl demografische Angaben als auch psychologische Konstrukte wie Technikaffinität, Vertrauen in KI-generierte Inhalte und Vorwissen über Künstliche Intelligenz. Zunächst wurden demografische Daten erfasst.azu zählten das Alter (metrisch, Freitexteingabe), das Geschlecht (nominal, Auswahl: männlich, weiblich, anderes mit Freitextoption), sowie der höchste allgemeinbildende (ordinal, 5 Stufen) und berufliche Bildungsabschluss (ordinal, 6 Stufen). Dann folgt ein Block zu der Nutzung von sozialen Medien. Hier wurde die Nutzungshäufigkeit (ordinal, 5 Stufen-Skala von „nie“ bis „täglich“) sowie genutzte Plattformen (nominal, Mehrfachauswahl) und Hauptnutzungsmotive (ebenfalls nominal, Mehrfachauswahl) abgefragt. Diese Kategorien basierten auf aktuellen Social-Media-Statistiken nach Röhl (2024). Für die Erfassung des Vorwissens zur Künstlichen Intelligenz verwenden wir eine metrische 5-Punkt-Skala, von 1 = sehr wenig bis 5= sehr viel. Außerdem wird das Misstrauen gegenüber von KI generierten Informationen auch mit einer metrischen Skala von 1 bis 5 gemessen. Mithilfe der Affinity for Technology Interaction Scale (ATI-Skala) nach Franke et al. (2019) wird die Technikaffinität der Teilnehmer erfasst. Die Skala besteht aus 9 Items, die auf einer 6-Stufen metrischen Likert-Skala beantwortet wurden. Zusätlich wurde ein Item zur Technikbereitschaft us der Kurzskala von Neyer et al. (2016) verwendet: „Hinsichtlich technischer Neuentwicklungen bin ich sehr neugierig“, beantwortet auf einer 5-Stufen metrischen Skala (1 = stimmt gar nicht bis 5 = stimmt völlig).

### Geplante Analysen

Wir planen mindestens einen t-Test und eine Korrelation, um Daten für unsere Forschungsfragen zu sammeln. Beim unserem t-Test können wir die beiden Szenarien miteinander vergleichen. Dabei ist die unabhängige Variable das genaue Szenario, bedeutet entweder mit oder ohne KI-Unterstützung. Die abhängige Variable ist die Bewertung, ob die Information stimmt. Dafür benutzen wir einen paired t-Test. Die Proband\*innen werden einmal nach dem Durchlauf ohne KI-Unterstützung und einmal nach dem Durchlauf mit KI-Unterstützung befragt. Ebenso könnte man untersuchen ob eine Gruppe mit hohem Vorwissen anders bewertet als eine Gruppe mit wenig Vorwissen. Die unabhängige Variable ist hier die Gruppe, in der sich die Proband*innen befinden*. Die abhängige Variable ist erneut die Bewertung. Dies wäre ein independent t-test. Des Weiteren untersuchen die Korrelation zwischen Vorkenntnissen und Ergebnissen, sowie zwischen Alter und den Ergebnissen. Dabei gehen wir davon aus, dass Personen mit hohen Vorkenntnissen öfter richtig liegen. Bei der zweiten Korrelation ist die Vermutung, dass jüngere Teilnehmer\*innen die Desinformation besser erkennen. Bei der ersten Korrelation sind die Variablen: 1. Vorkenntnisse und 2. Bewertungen. Bei der zweiten Korrelation sind es 1. Alter und 2. Bewertungen.

# Ergebnisse

## Tabellen: deskriptive Statistik

```{r}
#| echo: false
#| message: false
#| warning: false


library(dplyr)
library(tidyr)
library(knitr)
library(ggplot2)


studiendaten <- read.csv("data_combined.csv", encoding = "UTF-8")

studiendaten <- studiendaten %>%
  mutate(
    korrektheit_ohne_ki = decision_correct_rate_B,
    korrektheit_ki_gesamt = rowMeans(select(., decision_correct_rate_R, decision_correct_rate_E), na.rm = TRUE)
  )

metrische_variablen_alter_ki <- studiendaten %>%
  select(age, aiknowledge)
statistik_metrisch_alter_ki <- metrische_variablen_alter_ki %>%
  summarise(across(everything(), list(
    n = ~sum(!is.na(.)),
    mean = ~mean(., na.rm = TRUE),
    median = ~median(., na.rm = TRUE),
    min = ~min(., na.rm = TRUE),
    max = ~max(., na.rm = TRUE),
    sd = ~sd(., na.rm = TRUE)
  ), .names = "{.col}_{.fn}")) %>%
  pivot_longer(
    cols = everything(),
    names_to = c("Variable", "Statistik"),
    names_sep = "_(?=[^_]+$)"
  ) %>%
  mutate(
    Wert = if_else(
      Statistik != "n",
      round(value, 2), 
      round(value, 0)  
    )
  ) %>%
  select(Variable, Statistik, Wert) %>%
  pivot_wider(
    names_from = Variable,
    values_from = Wert
  ) %>%
  rename(Statistik = Statistik)


stat_order <- c("n", "mean", "median", "min", "max", "sd")
statistik_metrisch_alter_ki$Statistik <- factor(statistik_metrisch_alter_ki$Statistik, levels = stat_order)
statistik_metrisch_alter_ki <- statistik_metrisch_alter_ki %>%
  arrange(Statistik)

knitr::kable(
  statistik_metrisch_alter_ki,
  caption = "Alter und KI-Vorkenntnisse",
  col.names = c("Statistik", "Alter", "KI-Vorkenntnisse")
)

korrektheits_variablen <- studiendaten %>%
  select(korrektheit_ohne_ki, korrektheit_ki_gesamt)


statistik_korrektheit_prozent <- korrektheits_variablen %>%
  summarise(across(everything(), list(
    n = ~sum(!is.na(.)),
    mean = ~mean(., na.rm = TRUE),
    median = ~median(., na.rm = TRUE),
    min = ~min(., na.rm = TRUE),
    max = ~max(., na.rm = TRUE),
    sd = ~sd(., na.rm = TRUE)
  ), .names = "{.col}_{.fn}")) %>%
  pivot_longer(
    cols = everything(),
    names_to = c("Variable", "Statistik"),
    names_sep = "_(?=[^_]+$)"
  ) %>%
  mutate(
    Wert = if_else(
      Statistik != "n",
      round(value*100, 2), 
      round(value, 0)  
    )
  ) %>%
  select(Variable, Statistik, Wert) %>%
  pivot_wider(
    names_from = Variable,
    values_from = Wert
  ) %>%
  rename(Statistik = Statistik)


statistik_korrektheit_prozent$Statistik <- factor(statistik_korrektheit_prozent$Statistik, levels = stat_order)
statistik_korrektheit_prozent <- statistik_korrektheit_prozent %>%
  arrange(Statistik)


knitr::kable(
  statistik_korrektheit_prozent,
  caption = "Korrektheit der Entscheidungen (in Prozent)",
  col.names = c("Statistik", "ohne KI", "mit KI")
)


```

## Mittelwertvergleich

```{r}
#| echo: false
#| message: false
#| warning: false

# Pakete laden
library(ggplot2)
library(dplyr)
library(tidyr)
library(Hmisc)

# Daten einlesen
studiendaten2 <- read.csv("data_combined.csv", encoding = "UTF-8")

# Neue Spalten mit Korrektheit berechnen
studiendaten2 <- studiendaten2 %>%
  mutate(
    korrektheit_ohne_ki = decision_correct_rate_B,
    korrektheit_ki_gesamt = rowMeans(select(., decision_correct_rate_R, decision_correct_rate_E), na.rm = TRUE)
  )

# Daten ins Long-Format bringen für ggplot
data_long <- studiendaten2 %>%
  select(korrektheit_ohne_ki, korrektheit_ki_gesamt) %>%
  pivot_longer(cols = everything(), names_to = "Bedingung", values_to = "Korrektheit") %>%
  mutate(
    Bedingung = recode(Bedingung,
                       "korrektheit_ohne_ki" = "Ohne KI",
                       "korrektheit_ki_gesamt" = "Mit KI"),
    Korrektheit = Korrektheit * 100  # Falls deine Werte in [0,1] sind
  )

# Plot erstellen
ggplot(data_long, aes(x = Bedingung, y = Korrektheit, color = Bedingung)) +
  geom_jitter(width = 0.2, alpha = 0.5) +
  stat_summary(fun = mean, geom = "point", size = 4, shape = 18, color = "black") +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 0.2) +
  labs(title = "Vergleich der Entscheidungskorrektheit",
       y = "Korrektheit (%)", x = "Bedingung") +
  theme_minimal()

```

## Korrelationsanalyse

```{r}
#| echo: false
#| message: false
#| warning: false
#| results: 'hide'

# Pakete laden
library(ggplot2)
library(dplyr)
library(tidyr)
library(Hmisc)

# Daten einlesen
studiendaten3 <- read.csv("data_combined.csv", encoding = "UTF-8")

# Neue Spalten mit Korrektheit berechnen
studiendaten3 <- studiendaten3 %>%
  mutate(
    vorwissen_ki = aiknowledge,
    korrektheit_ki_gesamt = rowMeans(select(., decision_correct_rate_R, decision_correct_rate_E), na.rm = TRUE)
  )

cor.test(studiendaten3$aiknowledge,studiendaten3$korrektheit_ki_gesamt, method = "pearson")


# Plot erstellen
ggplot(studiendaten3, aes(x = vorwissen_ki, y = korrektheit_ki_gesamt)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(title = "Zusammenhang zwischen Vorwissen von KI und Korrektheit mit KI",
       x = "Vorwissen KI",
       y = "Korrektheit mit KI")

```

```{r}
#| echo: false
#| message: false
#| warning: false
#| results: 'hide'

# Pakete laden
library(ggplot2)
library(dplyr)
library(tidyr)
library(Hmisc)

# Daten einlesen
studiendaten4 <- read.csv("data_combined.csv", encoding = "UTF-8")

# Neue Spalten mit Korrektheit berechnen
studiendaten4 <- studiendaten4 %>%
  mutate(
    alter = age,
    korrektheit_gesamt = rowMeans(select(., decision_correct_rate_R, decision_correct_rate_E, decision_correct_rate_B), na.rm = TRUE)
  )


cor.test(studiendaten4$age,studiendaten4$korrektheit_gesamt, method = "pearson")

# Plot erstellen
ggplot(studiendaten3, aes(x = age, y = studiendaten4$korrektheit_gesamt)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(title = "Zusammenhang zwischen Alter und Korrektheit",
       x = "Alter",
       y = "Korrektheit")

```

## Stichprobe

An unserer Studie zu “KI-Unterstützungssysteme bei der Erkennung von Falschinformationen auf sozialen Medien unter Einfluss unterschiedlicher kognitiver Belastung der Beiträge” nahmen (*N* = 176) Probanden teil. Davon gaben 48 % der Teilnehmenden an, dass sie männlich seien, 51% gaben an, dass sie weiblich seien und 1 % gab an, weder dem männlichen noch dem weiblichen Geschlecht zu zugehören. Bei den Teilnehmenden waren Personen aus verschiedenen Altersgruppen vertreten, wobei das durchschnittliche Alter (*M* = 24,94) betrug bei einer Standardabweichung von (*SD* = 10,60). Die Mehrheit der an der Studie teilnehmenden stimmte der Aussage, dass sie bei Informationen, die von KI erzeugt werden, eher misstrauisch sind, tendenziell zu (*M* = 3,65). Gleichzeitig schätzten sie ihr eigenes Vorwissen und Ihre Vorerfahrungen zum Thema KI als eher mittelmäßig ein (*M* = 3,11). Darüber hinaus zeigte sich eine tendenzielle Zustimmung zur Aussage, dass es ihnen genüge, wenn ein technisches System funktioniere – unabhängig davon, wie oder warum (*M* = 3,49, *Mdn* = 4).

## Deskriptive Statistik

Bei der Betrachtung des Alters unserer (*N* = 176) Probanden, wobei der jüngste Teilnehmer 18 Jahre und der älteste 69 Jahre alt sind, ist aufgefallen, dass das Durchschnittsalter bei (*M* = 25.9) Jahren, mit einer Standardabweichung von (*SD* = 10.63), liegt. Der Median für das Alter unserer Probanden liegt bei (*Mdn* = 22) Jahren. Die KI Vorkenntnisse unserer Teilnehmer liegen in einem Bereich zwischen 1 und 5 Punkten. Der durchschnittlich erreichte Wert aller Teilnehmer liegt bei (*M* = 3.1 und) der Medianwert bei (*Mdn* = 3.0), mit einer Standardabweichung von (*SD* = .9). Somit erreichte die Mehrheit mehr als die Hälfte der maximal erreichbaren Punkte. Im weiteren Verlauf wurde die Anzahl korrekter Entscheidungen in Prozent, ohne und mit Unterstützung von KI untersucht. Bei der Betrachtung der Entscheidungsrichtigkeit ohne KI Unterstützung lag der Mindestprozentsatz der richtigen Entscheidungen bei 10 % und der Höchstprozentsatz bei 100 %. Im Durchschnitt haben die Probanden ohne KI Unterstützung (*M* = 73.1) % der Entscheidung richtig treffen können, mit einer Standardabweichung von 17.4 % und einem Median von (*Mdn* = 70) %. Unter Anwendung von KI stieg der Mindestprozentsatz für richtig getroffene Entscheidungen auf 20 %, mit dem Höchstprozentsatz weiterhin bei 100 % und die Standardabweichung ist auf (*SD* = 15.3) % gefallen. Mithilfe der KI konnte der Durchschnitt auf (*M* = 76.6) % und der Median auf (*Mdn* = 80) % gesteigert werden.

## Inferenzstatistik

Der Mittelwertvergleichsgraf “Vergleich der Entscheidungskorrektheit” betrachtet zwei metrische Variablen: Entscheidungskorrektheit mit KI, auf der linken Seite, und ohne KI, auf der rechten Seite des Graphen. Es lässt sich beobachten, dass die Box des Graphen für die Entscheidungskorrektheit mit KI im höchsten Quantil liegt, während die Box der Entscheidungskorrektheit ohne KI sich größtenteils im dritten Quantil befindet.

Bei der Betrachtung des Pearsongrafen “Zusammenhang zwischen Vorwissen von KI und Korrektheit mit KI” erhalten wir durch *t(#)* = .47 für die Differenz zwischen den beiden geprüften Variablen. Für diesen Test liegt *p* = .63 und die Korrelation *cor* = .03 und die Regressionslinie verläuft linear positiv.

Betrachte man allerdings den Pearsongrafen “Zusammenhang zwischen Alter und Korrektheit” ist die Differenz der beiden geprüften Variablen mit *t(#)* = .08 deutlich unter dem vom vorherigen Grafen. Der p-Wert *p* = .94 ist deutlich höher im Vergleich zum ersten Pearsongrafen, dementsprechend ist auch die Korrelation mit *cor* = .006 deutlich geringer. Die Regressionslinie verläuft marginal linear positiv.


# Diskussion

## *(Diskussion einfügen)*

# Anhänge

## Anhang 1 - Rekrutierungstext (Online)

**"Hilf uns Fake News zu bekämpfen!**

Immer mehr Menschen werden durch Fake News im Internet beeinflusst. Um das zu verhindern, untersuchen wir in unserer Online-Studie wie man Künstliche Intelligenz dazu nutzen kann, um Falschinformationen zu erkennen und Menschen davor zu warnen. Dafür brauchen wir jetzt deine Hilfe.

**🕒 Dauer:  ca. 1h**

🎁 **Vergütung:** **1 VP-Stunde**

✅ **Bedingungen:**

-   *min. 18 Jahre alt*

-   *nicht Teilnehmer\*in im Kurs "Statistik und Methoden der Nutzerforschung"*

-   *keine Teilnahme per Smartphone*

🔗 **Teilnahmelink:**

[Link zur Studie](https://dsslab.hciuse.sh/study/pilot?groupId=gr-b5)

Wir freuen uns, wenn du uns unterstützt :)."

"Tommy05lol"

"DefoNotAlex"

"Matt1s1234"

"NeiUt"

"jakolex03"
